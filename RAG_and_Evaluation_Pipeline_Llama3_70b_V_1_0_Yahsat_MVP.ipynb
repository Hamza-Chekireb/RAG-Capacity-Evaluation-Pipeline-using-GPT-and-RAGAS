{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/RAG-Capacity-Evaluation-Pipeline-using-GPT-and-RAGAS/blob/main/RAG_and_Evaluation_Pipeline_Llama3_70b_V_1_0_Yahsat_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5PjNQjkfTx-"
      },
      "source": [
        "### 1.Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQmjUpgWPUd"
      },
      "source": [
        "#### 1.1. Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation of the necessary libraries"
      ],
      "metadata": {
        "id": "9ScEr1kJFToN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45wVu-20fKNk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install sentence_transformers\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install chromadb\n",
        "!pip install openai\n",
        "!pip install langchain_nvidia_ai_endpoints\n",
        "!pip install fastapi pyngrok uvicorn nest-asyncio\n",
        "!pip install openai\n",
        "!pip install ragas\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJXB1da9WSl4"
      },
      "source": [
        "#### 1.2. Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsykH32kfbuE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline # AutoModelForCausalLM is used to add layer for application like QA\n",
        "import langchain\n",
        "from langchain import HuggingFacePipeline\n",
        "from openai import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "# import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wHz92Qwx7HV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPYngq6CjLb9"
      },
      "outputs": [],
      "source": [
        "# ChatNVIDIA.get_available_models()\n",
        "# llm.get_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHWM09U2gHeZ"
      },
      "source": [
        "### 2.Creation and Management of the Vector Database\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyooYIRGf9lw"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZewdYU_gl6O"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/Yahsat RAG/Documents/Bibliographie'\n",
        "DB_FAISS_PATH = 'vectorstore5/db_chroma5'\n",
        "\n",
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\n",
        "                                                   chunk_overlap=32)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "    # model_name=\"Alibaba-NLP/gte-large-en-v1.5\",\n",
        "    # model_name=\"sentence-transformers/all-roberta-large-v1\",\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': DEVICE, 'trust_remote_code': True} # Add trust_remote_code to allow execution of remote configuration\n",
        ")\n",
        "\n",
        "    # 4. Creat Vector Data Base\n",
        "    vdb = Chroma.from_documents(texts,embeddings,persist_directory=DB_FAISS_PATH)\n",
        "    # store the db in repertory\n",
        "    vdb.persist()\n",
        "    return vdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id1rN4mMgy9J"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "vdb = create_vector_db()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3fh4SvKkCGY"
      },
      "source": [
        "### 3.Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQRU9X4Zj7cA"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
        "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
        "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
        "else:\n",
        "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
        "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NVIDIA_API_KEY\"] = \"***********************************************\""
      ],
      "metadata": {
        "id": "0RcM8hVs2oRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "id": "BFWqp_AJ5ZMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM7gsNSakH-W"
      },
      "outputs": [],
      "source": [
        "# Test the API\n",
        "rag_llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", base_url = \"https://integrate.api.nvidia.com/v1\")\n",
        "# result = llm.invoke(\"Tell a fact about the moon.\")\n",
        "# print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViC1CAxkuaVB"
      },
      "outputs": [],
      "source": [
        "result = rag_llm.invoke(\"Tell a fact about the moon.\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP4yjY3GtdwO"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVJARL1lo8Uc"
      },
      "source": [
        "**For Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg7OwR4DoM04"
      },
      "outputs": [],
      "source": [
        "def rag_pipeline(query):\n",
        "\n",
        "  docs = vdb.similarity_search(query,k=3)\n",
        "  context = \"\\n\\n\".join([doc.page_content for doc in docs]).replace(\"{\",\"(\").replace(\"}\",\")\")\n",
        "  prompt_template = (\n",
        "      \"You are an AI assistant. Use the following context and conversation history to answer the question in a concise and direct way.\\n\"\n",
        "      f\"Context: {context}\\n\\n\"\n",
        "      \"Conversation History: {history}\\n\\n\"\n",
        "      \"User's Question: {query}\\n \"\n",
        "  )\n",
        "  prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"query\",\"history\"])\n",
        "  chain = LLMChain(\n",
        "      llm=rag_llm,\n",
        "      prompt=prompt,\n",
        "      verbose=False,\n",
        "      memory=memory\n",
        "  )\n",
        "  return {\"response\" : chain.run(query),\n",
        "            \"context\" : docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvI_TM5on8g"
      },
      "source": [
        "### 4.API for externe connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrE_-I65qIlx"
      },
      "outputs": [],
      "source": [
        "def final_result(query,context):\n",
        "  # docs = vdb.similarity_search(query,k=2)\n",
        "  # context = \"\\n\\n \".join([doc.page_content for doc in docs])\n",
        "  prompt_template = (\n",
        "    \"You are an AI assistant. Use the following context and conversation history to answer the question in a concise and direct way.\\n\"\n",
        "    f\"Context: {context}\\n\\n\"\n",
        "    \"Conversation History: {history}\\n\\n\"\n",
        "    \"User's Question: {query}\\n \"\n",
        "  )\n",
        "  prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"query\",\"history\"])\n",
        "  chain = LLMChain(\n",
        "    llm=rag_llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        "    memory=memory\n",
        "  )\n",
        "  return chain.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW3HE1Xz4TGo"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "class Item(BaseModel):\n",
        "    question: str\n",
        "    context: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcr-kbSsowFf"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post('/llama3-70b')\n",
        "def api(item: Item):\n",
        "    # Call your function and get the result\n",
        "    result = final_result(item.question, item.context)\n",
        "    return result  # Return the result as JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nt02NK_q772"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken 2i5bGtpF2hshSNabZb5pE0PPRKP_7fgcebbGhedWZPDx7kS9L\n",
        "# Allow nested asyncio loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za_fE2sUFnkU"
      },
      "source": [
        "### 5.Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPqecFUoRMRp"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# Get all the chunks\n",
        "loader = DirectoryLoader(DATA_PATH,\n",
        "                          glob='*.pdf',\n",
        "                          loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\n",
        "                                                chunk_overlap=32)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmFg8ds9Qxcv"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4TDBrJIIt4P"
      },
      "source": [
        "#### 5.1.Creating the dataset : **Generate questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXNlkyaznDv"
      },
      "source": [
        "**Generate a question for each context**: This part adds a question, generated by the ground_truth model for some selected contexts, to the evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZQUroDHGY3A"
      },
      "outputs": [],
      "source": [
        "#1. Generate questions shema\n",
        "question_schema = ResponseSchema(\n",
        "    name=\"question\",\n",
        "    description=\"A question about the context\",\n",
        "    type=\"string\"\n",
        ")\n",
        "\n",
        "context_schema = ResponseSchema(\n",
        "    name=\"context\",\n",
        "    description=\"The context related to the question\",\n",
        "    type=\"string\"\n",
        ")\n",
        "\n",
        "# Create a Structured Output Parser with both schemas\n",
        "parser = StructuredOutputParser.from_response_schemas([question_schema, context_schema])\n",
        "\n",
        "# Get the format instructions\n",
        "format_instructions = parser.get_format_instructions()\n",
        "print(format_instructions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeS-K4CYJVBs"
      },
      "outputs": [],
      "source": [
        "qa_template = \"\"\"You are a University professor creating a test for advanced students.\n",
        "For each context, generate a question about the context.\n",
        "Format the output as JSON with the following keys:\n",
        "- question\n",
        "- context\n",
        "Question: Generate a question about the given context\n",
        "Context: {context}\n",
        "format_instructions: {format_instructions}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mi5NFypKzoa"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(qa_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fouzhwALQyw"
      },
      "outputs": [],
      "source": [
        "prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRjW308ELRtk"
      },
      "outputs": [],
      "source": [
        "# messages = prompt_template.format_messages(context=docs[2].page_content,\n",
        "#                                            format_instructions=format_instructions)\n",
        "# messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kadj7YwCv1_G"
      },
      "outputs": [],
      "source": [
        "# OpenAI Modelfrom langchain.chat_models import ChatOpenAI\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] = \"**************************\"\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "ground_truth_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.1, max_tokens=1024 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx6-tXEKYjKJ"
      },
      "outputs": [],
      "source": [
        "# result = ground_truth_llm.invoke(messages)\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkGEspqdSuM5"
      },
      "outputs": [],
      "source": [
        "# output_dict = parser.parse(result.content)\n",
        "# output_dict\n",
        "# Markdown(output_dict['question'])\n",
        "# Markdown(output_dict['context'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgidGF93vcEU"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvj70Ztzyqlx"
      },
      "outputs": [],
      "source": [
        "def creat_qc(docs) -> list :\n",
        "  qc = []\n",
        "  random_docs = random.sample(docs, 15)\n",
        "  for doc in tqdm(random_docs):\n",
        "    messages = prompt_template.format_messages(context=doc.page_content.replace(\"{\",\"(\").replace(\"}\",\")\"),\n",
        "                                              format_instructions=format_instructions)\n",
        "    result = rag_llm.invoke(messages)\n",
        "    try :\n",
        "      output_dict = parser.parse(result.content)\n",
        "      qc.append(output_dict)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "  return qc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esDlnvboz6Vy"
      },
      "outputs": [],
      "source": [
        "qc = creat_qc(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KszDBpZ25bR"
      },
      "outputs": [],
      "source": [
        "qc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qr9SrXcFxgY"
      },
      "outputs": [],
      "source": [
        "# for i in tqdm(range(len(qac))):\n",
        "#   print()\n",
        "#   print(type(qac[i]))\n",
        "#   print(qac[i].keys())\n",
        "#   print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFMpPwEA1pWO"
      },
      "source": [
        "#### 5.2.Creating the dataset(Generate answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-zmmXE2aA3o"
      },
      "source": [
        "This part is dedicated to creating an answer for each context. The answer is the ground truth answer; in other words, it is the most accurate answer provided by the most accurate model (in our case, GPT-4). These answers will be used and compared with the answers generated by our RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pOrIXTu0xba"
      },
      "outputs": [],
      "source": [
        "answer_schema = ResponseSchema(name=\"answer\", description=\"The answer to the question\")\n",
        "parser = StructuredOutputParser.from_response_schemas([answer_schema])\n",
        "parser\n",
        "format_instructions = parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG2IX9PJ3lhb"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a University professor creating a test for advanced students.\n",
        "For each question and context, generate a answer.\n",
        "Format the output as JSON with the following keys:\n",
        "- answer\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "format_instructions: {format_instructions}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4w7b-Un4wc4"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY1f-G_d42SS"
      },
      "outputs": [],
      "source": [
        "# messages = prompt_template.format_messages(context=qac[0]['context'],\n",
        "#                                            question = qac[0]['question'],\n",
        "#                                            format_instructions=format_instructions)\n",
        "# print(messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeIIY3_s5STj"
      },
      "outputs": [],
      "source": [
        "# response = llm.invoke(messages)\n",
        "# print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CR4SdVy51q3"
      },
      "outputs": [],
      "source": [
        "# output_dict = parser.parse(response.content)\n",
        "# output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bb3I3hh6K-P"
      },
      "outputs": [],
      "source": [
        "# Adding the ground truth answers to the dataset\n",
        "def add_ground_truth_answers(qc) -> list:\n",
        "  for i in tqdm(qc):\n",
        "    messages = prompt_template.format_messages(context=i['context'],\n",
        "                                              question = i['question'],\n",
        "                                              format_instructions=format_instructions)\n",
        "    response = ground_truth_llm.invoke(messages)\n",
        "    try :\n",
        "      output_dict = parser.parse(response.content)\n",
        "      i['answer'] = output_dict['answer']\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "  qcg = qc\n",
        "  return qcg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTkrTnK5_4gu"
      },
      "source": [
        "#### 5.3. Create the evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Frj1sHc-rz3"
      },
      "outputs": [],
      "source": [
        "# # from datasets import Dataset\n",
        "# import pandas as pd\n",
        "# ground_truth_data = pd.read_csv('ground_truth.csv')\n",
        "# ground_truth_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpn8FUiq5mI7"
      },
      "outputs": [],
      "source": [
        "from re import escape\n",
        "qcg = pd.DataFrame(add_ground_truth_answers(qc))\n",
        "qcg_csv = qcg.to_csv('/content/qcg.csv', index=False, escapechar = '\\\\')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0MdFuY23BHd"
      },
      "outputs": [],
      "source": [
        "qcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC1UGILi9jFk"
      },
      "outputs": [],
      "source": [
        "qcg.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlpbyFDW_-E8"
      },
      "source": [
        "#### 5.4. Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HO-DbuOA8kD"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qnVpFDR_E4H"
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
        "from ragas.metrics.critique import harmfulness\n",
        "from ragas import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FUqhShgA0Zq"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJnwXKSfDKhz"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "qcg_dataset = Dataset.from_csv(\"/content/qcg.csv\")\n",
        "qcg_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUduzNitctlw"
      },
      "source": [
        "The objective of this function is to create a dataset to be used with Ragas and have this schema.\n",
        "\n",
        "```\n",
        "{\n",
        "    \"context\": [\n",
        "        \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize and the only person to win Nobel Prizes in two different scientific fields.\",\n",
        "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\n",
        "    ],\n",
        "    \"questions\": [\n",
        "        \"Who was the first woman to win a Nobel Prize?\",\n",
        "        \"What is the Eiffel Tower named after?\"\n",
        "    ],\n",
        "    \"ground_truth_answers\": [\n",
        "        \"Marie Curie was the first woman to win a Nobel Prize.\",\n",
        "        \"The Eiffel Tower is named after Gustave Eiffel.\"\n",
        "    ],\n",
        "    \"generated_answers\": [\n",
        "        \"Marie Curie was the first woman to win a Nobel Prize.\",\n",
        "        \"The Eiffel Tower is named after the engineer who built it, Gustave Eiffel.\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si6unZLSLBem"
      },
      "outputs": [],
      "source": [
        "def create_ragas_eval_dataset(rag_pipeline, qcg_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(qcg_dataset):\n",
        "    # Anser generated by the pipeline\n",
        "    answer = rag_pipeline(row['question'])\n",
        "    rag_dataset.append({\n",
        "        'question' : row['question'],\n",
        "        'answer' : answer['response'],\n",
        "        'ground_truth' : row['answer'],\n",
        "        \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
        "    })\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return eval_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zif-N-31CH-J"
      },
      "outputs": [],
      "source": [
        "eval_dataset = create_ragas_eval_dataset(rag_pipeline, qcg_dataset)\n",
        "eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJzhgqU8YPnf"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade ragas  # Ensure you have the latest version of ragas\n",
        "# ! pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOre1Q7-qHrc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBAr3CRUX61L"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "from ragas import evaluate, metrics, RunConfig\n",
        "from ragas.metrics import  answer_relevancy, context_recall, context_precision, faithfulness, ContextRelevancy\n",
        "\n",
        "def evaluate_ragas_dataset(ragas_dataset):\n",
        "  result = evaluate(\n",
        "      ragas_dataset,\n",
        "      metrics = [\n",
        "      answer_relevancy,\n",
        "      context_recall,\n",
        "      context_precision,\n",
        "      faithfulness\n",
        "      ],\n",
        "      llm = ground_truth_llm,\n",
        "      # Pass raise_exceptions=False to get more information about the error\n",
        "      raise_exceptions=False\n",
        "  )\n",
        "  return result\n",
        "\n",
        "evaluation = evaluate_ragas_dataset(eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNBQBStxCOjB"
      },
      "outputs": [],
      "source": [
        "#'sentence-transformers/all-MiniLM-L6-v2'chenks = 512\n",
        "# Eval Model = \"gpt-4o\"\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaRFGe564jQy"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation result, which may contain error messages : #'sentence-transformers/all-MiniLM-L6-v2'chenks = 1024\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJhxy_InBuNP"
      },
      "outputs": [],
      "source": [
        "# sentence-transformers/all-roberta-large-v1\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR-VqPNThRCG"
      },
      "outputs": [],
      "source": [
        "# Embedding model used : \"Alibaba-NLP/gte-large-en-v1.5\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iITzwgdJUvtu"
      },
      "outputs": [],
      "source": [
        "# Embedding model used : #'sentence-transformers/all-MiniLM-L6-v2'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ic5jUv5w0gi"
      },
      "source": [
        "**Answer Relevancy (0.9736)**: Indicates that the answers generated are very relevant to the user's queries.\n",
        "\n",
        "**Context Recall (0.7273)**: Shows that the system is retrieving a significant portion of relevant context but may still miss some relevant pieces.\n",
        "\n",
        "**Context Precision (0.9231)**: Suggests that the majority of the retrieved context is relevant, with minimal irrelevant information included.\n",
        "\n",
        "**Faithfulness (0.8654)**: Implies that the generated responses are mostly accurate and true to the source information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAqoda7cUvba"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cqvI_TM5on8g"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMtmxZ7WZD0ua5oBLtZbNO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}